<!DOCTYPE html>
<html lang="zh-TW">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Digital Human</title>
    <link rel="stylesheet" href="style.css">
</head>

<body>

    <div class="container">
        <div class="avatar-wrapper">
            <img src="avatar.jpg" alt="Digital Human" id="avatar">
            <div class="pulse-ring"></div>
        </div>

        <div class="status-box">
            <p id="status-text">é»æ“Šéº¥å…‹é¢¨é–‹å§‹å°è©±...</p>
        </div>

        <!-- Voice Mode UI -->
        <button id="mic-btn" onclick="toggleListening()">
            ğŸ¤ é–‹å§‹èªªè©±
        </button>

        <!-- Text Mode UI (Hidden by default) -->
        <div id="text-input-area" class="hidden">
            <input type="text" id="user-input" placeholder="æ‚¨çš„ç€è¦½å™¨ä¸æ”¯æ´èªéŸ³ï¼Œè«‹è¼¸å…¥æ–‡å­—..." />
            <button id="send-btn" onclick="handleTextSubmit()">ç™¼é€</button>
        </div>
    </div>

    <script>
        const GAS_URL = "https://script.google.com/macros/s/AKfycbyeVOfZ6B1LdOWQb2bibUubSmo-mrX8GBcKnxwHz2yytLAN5vCjQf6OD95PniC3FD9_FA/exec";

        const avatar = document.getElementById('avatar');
        const statusText = document.getElementById('status-text');
        const micBtn = document.getElementById('mic-btn');
        const textInputArea = document.getElementById('text-input-area');
        const userInput = document.getElementById('user-input');

        let isListening = false;
        let recognition = null;

        // Check Browser Support
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

        if (SpeechRecognition) {
            // Support Voice
            setupVoiceRecognition(SpeechRecognition);
        } else {
            // No Voice Support -> Switch to Text Mode
            switchToTextMode("æ‚¨çš„ç€è¦½å™¨ä¸æ”¯æ´èªéŸ³è¼¸å…¥ï¼Œå·²åˆ‡æ›ç‚ºæ–‡å­—æ¨¡å¼ã€‚");
        }

        function setupVoiceRecognition(SpeechRecognition) {
            recognition = new SpeechRecognition();
            recognition.lang = 'zh-TW';
            recognition.interimResults = false;

            recognition.onstart = () => {
                isListening = true;
                statusText.innerText = "æ­£åœ¨è½æ‚¨èªªè©±...";
                micBtn.innerText = "ğŸ›‘ åœæ­¢";
                micBtn.classList.add("listening");
                avatar.classList.add("listening-mode");
            };

            recognition.onend = () => {
                if (statusText.innerText === "æ­£åœ¨è½æ‚¨èªªè©±...") {
                    resetUI();
                }
            };

            recognition.onresult = async (event) => {
                const transcript = event.results[0][0].transcript;
                statusText.innerText = `æ‚¨èªª: "${transcript}"`;
                isListening = false;
                recognition.stop();
                await sendToAI(transcript);
            };

            recognition.onerror = (event) => {
                console.error(event.error);
                if (event.error === 'not-allowed' || event.error === 'service-not-allowed') {
                    switchToTextMode("ç„¡æ³•å­˜å–éº¥å…‹é¢¨ï¼Œå·²åˆ‡æ›ç‚ºæ–‡å­—æ¨¡å¼ã€‚");
                } else {
                    statusText.innerText = "èªéŸ³è¾¨è­˜éŒ¯èª¤ï¼Œè«‹é‡è©¦æˆ–æª¢æŸ¥éº¥å…‹é¢¨ã€‚";
                    resetUI();
                }
            };
        }

        function switchToTextMode(msg) {
            statusText.innerText = msg;
            micBtn.style.display = 'none'; // Hide Mic
            textInputArea.classList.remove('hidden'); // Show Text Input
            textInputArea.style.display = 'flex';
        }

        function toggleListening() {
            if (!recognition) return;
            if (isListening) {
                recognition.stop();
                resetUI();
            } else {
                recognition.start();
            }
        }

        async function handleTextSubmit() {
            const text = userInput.value.trim();
            if (!text) return;
            userInput.value = '';
            statusText.innerText = `æ‚¨è¼¸å…¥: "${text}"`;
            await sendToAI(text);
        }

        function resetUI() {
            isListening = false;
            micBtn.innerText = "ğŸ¤ é–‹å§‹èªªè©±";
            micBtn.classList.remove("listening");
            avatar.classList.remove("listening-mode");
        }

        async function sendToAI(message) {
            statusText.innerText = "æ€è€ƒä¸­...";
            avatar.classList.add("thinking-mode");

            try {
                const response = await fetch(GAS_URL, {
                    method: 'POST',
                    body: JSON.stringify({ message: message })
                });
                const data = await response.json();

                if (data.reply) {
                    statusText.innerText = `AI: "${data.reply}"`;
                    speak(data.reply);
                } else if (data.error) {
                    throw new Error("Backend Error: " + data.error);
                } else {
                    throw new Error("No reply key in response: " + JSON.stringify(data));
                }
            } catch (e) {
                console.error(e);
                statusText.innerText = "é€£ç·šå¤±æ•—: " + e.message;
            } finally {
                avatar.classList.remove("thinking-mode");
            }
        }

        function speak(text) {
            // Mobile browsers often block audio unless triggered by user interaction.
            // Since this comes from an async fetch, it might be blocked on some iOS devices.
            // But usually, if the user initiated the flow (click send/mic), it works.
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.lang = "zh-TW";

            const voices = window.speechSynthesis.getVoices();
            console.log(voices);
            const zhVoice = voices.find(v => v.lang.includes('zh') && (v.name.includes('Google') || v.name.includes('Female')));
            if (zhVoice) utterance.voice = zhVoice;

            utterance.onstart = () => {
                avatar.classList.add("speaking-mode");
            };

            utterance.onend = () => {
                avatar.classList.remove("speaking-mode");
                resetUI();
            };

            window.speechSynthesis.speak(utterance);
        }
    </script>
</body>

</html>
