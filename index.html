<!DOCTYPE html>
<html lang="zh-TW">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Digital Human (OpenAI Voice)</title>
    <!-- Use standard style.css for GitHub Pages compatibility -->
    <link rel="stylesheet" href="style.css">
</head>

<body>

    <div class="container">
        <div class="avatar-wrapper">
            <!-- ä½¿ç”¨ DiceBear ä½œç‚ºé è¨­é ­åƒï¼Œæ‚¨å¯ä»¥æ›¿æ›æˆè‡ªå·±çš„ avatar.jpg -->
            <img src="https://api.dicebear.com/7.x/avataaars/svg?seed=Felix" alt="Digital Human" id="avatar">
            <div class="pulse-ring"></div>
        </div>

        <div class="status-box">
            <p id="status-text">é»æ“Šéº¥å…‹é¢¨é–‹å§‹å°è©±...</p>
        </div>

        <!-- Voice Mode UI -->
        <button id="mic-btn" onclick="toggleListening()">
            ğŸ¤ é–‹å§‹èªªè©±
        </button>

        <!-- Text Mode UI (Hidden by default) -->
        <div id="text-input-area" class="hidden">
            <input type="text" id="user-input" placeholder="æ‚¨çš„ç€è¦½å™¨ä¸æ”¯æ´èªéŸ³ï¼Œè«‹è¼¸å…¥æ–‡å­—..." />
            <button id="send-btn" onclick="handleTextSubmit()">ç™¼é€</button>
        </div>
    </div>

    <script>
        // *** é‡è¦ï¼šè«‹å°‡æ­¤ç¶²å€æ›¿æ›ç‚ºæ‚¨éƒ¨ç½²çš„ Google Apps Script ç¶²å€ ***
        const GAS_URL = "https://script.google.com/macros/s/AKfycbycRHXi7PqLpQ6V6cZmeSigwjdG8fHQ8OK8ECBW2h1EV1QwEZYuaWGigKfN0DGlPGTL6A/exec";

        const avatarWrapper = document.querySelector('.avatar-wrapper');
        const statusText = document.getElementById('status-text');
        const micBtn = document.getElementById('mic-btn');
        const textInputArea = document.getElementById('text-input-area');
        const userInput = document.getElementById('user-input');

        let isListening = false;
        let recognition = null;
        let currentAudio = null;

        // Check Browser Support
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

        if (SpeechRecognition) {
            setupVoiceRecognition(SpeechRecognition);
        } else {
            switchToTextMode("æ‚¨çš„ç€è¦½å™¨ä¸æ”¯æ´èªéŸ³è¼¸å…¥ï¼Œå·²åˆ‡æ›ç‚ºæ–‡å­—æ¨¡å¼ã€‚");
        }

        function setupVoiceRecognition(SpeechRecognition) {
            recognition = new SpeechRecognition();
            recognition.lang = 'zh-TW';
            recognition.interimResults = false;

            recognition.onstart = () => {
                isListening = true;
                statusText.innerText = "æ­£åœ¨è½æ‚¨èªªè©±...";
                micBtn.innerText = "ğŸ›‘ åœæ­¢";
                micBtn.classList.add("listening");
                updateState('listening');
            };

            recognition.onend = () => {
                if (statusText.innerText === "æ­£åœ¨è½æ‚¨èªªè©±...") {
                    resetUI();
                }
            };

            recognition.onresult = async (event) => {
                const transcript = event.results[0][0].transcript;
                statusText.innerText = `æ‚¨èªª: "${transcript}"`;
                isListening = false;
                recognition.stop();
                await sendToAI(transcript);
            };

            recognition.onerror = (event) => {
                console.error(event.error);
                if (event.error === 'not-allowed' || event.error === 'service-not-allowed') {
                    switchToTextMode("ç„¡æ³•å­˜å–éº¥å…‹é¢¨ï¼Œå·²åˆ‡æ›ç‚ºæ–‡å­—æ¨¡å¼ã€‚");
                } else {
                    statusText.innerText = "èªéŸ³è¾¨è­˜éŒ¯èª¤ï¼Œè«‹é‡è©¦ã€‚";
                    resetUI();
                }
            };
        }

        function switchToTextMode(msg) {
            statusText.innerText = msg;
            micBtn.style.display = 'none';
            textInputArea.classList.remove('hidden');
        }

        function toggleListening() {
            if (!recognition) return;
            // å¦‚æœæ­£åœ¨æ’­æ”¾éŸ³è¨Šï¼Œé»æ“ŠæŒ‰éˆ•æ‡‰è©²åœæ­¢æ’­æ”¾
            if (currentAudio && !currentAudio.paused) {
                currentAudio.pause();
                currentAudio = null;
                resetUI();
                return;
            }

            if (isListening) {
                recognition.stop();
                resetUI();
            } else {
                recognition.start();
            }
        }

        async function handleTextSubmit() {
            const text = userInput.value.trim();
            if (!text) return;
            userInput.value = '';
            statusText.innerText = `æ‚¨è¼¸å…¥: "${text}"`;
            await sendToAI(text);
        }

        function resetUI() {
            isListening = false;
            micBtn.innerText = "ğŸ¤ é–‹å§‹èªªè©±";
            micBtn.classList.remove("listening");
            avatarWrapper.className = 'avatar-wrapper'; // Remove all state classes
        }

        function updateState(state) {
            avatarWrapper.className = 'avatar-wrapper'; // Reset
            if (state) avatarWrapper.classList.add('state-' + state);
        }

        async function sendToAI(message) {
            statusText.innerText = "æ€è€ƒä¸­...";
            updateState('thinking');

            try {
                const response = await fetch(GAS_URL, {
                    method: 'POST',
                    body: JSON.stringify({ message: message })
                });
                const data = await response.json();

                if (data.error) {
                    throw new Error("Backend Error: " + data.error);
                }

                if (data.reply) {
                    statusText.innerText = `AI: "${data.reply}"`;

                    // æª¢æŸ¥æ˜¯å¦æœ‰éŸ³è¨Šè³‡æ–™
                    if (data.audioData) {
                        playAudio(data.audioData);
                    } else {
                        // Fallback to browser TTS if no audioData
                        console.warn("No audioData received, using fallback.");
                        speakFallback(data.reply);
                    }
                }
            } catch (e) {
                console.error(e);
                statusText.innerText = "é€£ç·šå¤±æ•—: " + e.message;
                resetUI();
            }
        }

        function playAudio(base64Data) {
            updateState('speaking');
            micBtn.innerText = "ğŸ›‘ åœæ­¢æ’­æ”¾";

            if (currentAudio) {
                currentAudio.pause();
                currentAudio = null;
            }

            currentAudio = new Audio("data:audio/mp3;base64," + base64Data);

            currentAudio.onended = () => {
                currentAudio = null;
                resetUI();
            };

            currentAudio.play().catch(e => {
                console.error("Audio play failed:", e);
                statusText.innerText = "è‡ªå‹•æ’­æ”¾è¢«é˜»æ“‹ï¼Œè«‹é»æ“Šç•«é¢é‡è©¦ã€‚";
                resetUI();
            });
        }

        function speakFallback(text) {
            // åŸæœ‰çš„ç€è¦½å™¨èªéŸ³åˆæˆä½œç‚ºå‚™æ¡ˆ
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.lang = "zh-TW";
            utterance.onstart = () => updateState('speaking');
            utterance.onend = () => resetUI();
            window.speechSynthesis.speak(utterance);
        }
    </script>
</body>

</html>
