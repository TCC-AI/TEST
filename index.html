<!DOCTYPE html>
<html lang="zh-TW">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Digital Human</title>
    <link rel="stylesheet" href="style.css">
</head>

<body>

    <div class="container">
        <div class="avatar-wrapper">
            <img src="avatar.jpg" alt="Digital Human" id="avatar">
            <div class="pulse-ring"></div>
        </div>

        <div class="status-box">
            <p id="status-text">é»æ“Šéº¥å…‹é¢¨é–‹å§‹å°è©±...</p>
        </div>

        <button id="mic-btn" onclick="toggleListening()">
            ğŸ¤ é–‹å§‹èªªè©±
        </button>
    </div>

    <script>
        const GAS_URL = "https://script.google.com/macros/s/AKfycbyeVOfZ6B1LdOWQb2bibUubSmo-mrX8GBcKnxwHz2yytLAN5vCjQf6OD95PniC3FD9_FA/exec";

        const avatar = document.getElementById('avatar');
        const statusText = document.getElementById('status-text');
        const micBtn = document.getElementById('mic-btn');
        let isListening = false;

        // Speech Recognition Setup
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!SpeechRecognition) {
            alert("æ‚¨çš„ç€è¦½å™¨ä¸æ”¯æ´èªéŸ³è¾¨è­˜ï¼Œè«‹ä½¿ç”¨ Chrome æˆ– Edgeã€‚");
        }
        const recognition = new SpeechRecognition();
        recognition.lang = 'zh-TW';
        recognition.interimResults = false;

        recognition.onstart = () => {
            isListening = true;
            statusText.innerText = "æ­£åœ¨è½æ‚¨èªªè©±...";
            micBtn.innerText = "ğŸ›‘ åœæ­¢";
            micBtn.classList.add("listening");
            avatar.classList.add("listening-mode");
        };

        recognition.onend = () => {
            if (isListening) { // If it stopped naturally but we didn't get result yet or just finished
                // We rely on 'result' event to trigger next step. 
                // If we forcefully stopped, isListening would be false.
            }
            // Reset UI if not processing
            if (statusText.innerText === "æ­£åœ¨è½æ‚¨èªªè©±...") {
                resetUI();
            }
        };

        recognition.onresult = async (event) => {
            const transcript = event.results[0][0].transcript;
            statusText.innerText = `æ‚¨èªª: "${transcript}"`;
            isListening = false;
            recognition.stop();

            // Send to Backend
            await sendToAI(transcript);
        };

        recognition.onerror = (event) => {
            console.error(event.error);
            statusText.innerText = "ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹é‡è©¦ã€‚";
            resetUI();
        };

        function toggleListening() {
            if (isListening) {
                recognition.stop();
                resetUI();
            } else {
                recognition.start();
            }
        }

        function resetUI() {
            isListening = false;
            micBtn.innerText = "ğŸ¤ é–‹å§‹èªªè©±";
            micBtn.classList.remove("listening");
            avatar.classList.remove("listening-mode");
        }

        async function sendToAI(message) {
            statusText.innerText = "æ€è€ƒä¸­...";
            avatar.classList.add("thinking-mode");

            try {
                const response = await fetch(GAS_URL, {
                    method: 'POST',
                    body: JSON.stringify({ message: message }) // No-mode CORS or handled by GAS
                });
                const data = await response.json();

                if (data.reply) {
                    statusText.innerText = `AI: "${data.reply}"`;
                    speak(data.reply);
                } else if (data.error) {
                    throw new Error("Backend Error: " + data.error);
                } else {
                    throw new Error("No reply key in response: " + JSON.stringify(data));
                }
            } catch (e) {
                console.error(e);
                statusText.innerText = "é€£ç·šå¤±æ•—: " + e.message; // Show exact error
            } finally {
                avatar.classList.remove("thinking-mode");
            }
        }

        function speak(text) {
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.lang = "zh-TW";

            // Try to select a better voice if available (e.g. Google åœ‹èª)
            const voices = window.speechSynthesis.getVoices();
            console.log(voices); // For debugging
            // Simple heuristic to find a female Chinese voice
            const zhVoice = voices.find(v => v.lang.includes('zh') && (v.name.includes('Google') || v.name.includes('Female')));
            if (zhVoice) utterance.voice = zhVoice;

            utterance.onstart = () => {
                avatar.classList.add("speaking-mode");
            };

            utterance.onend = () => {
                avatar.classList.remove("speaking-mode");
                resetUI();
            };

            window.speechSynthesis.speak(utterance);
        }
    </script>
</body>

</html>
